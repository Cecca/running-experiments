\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage{todonotes}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}

\newcommand{\matodo}[1]{\todo[inline, color=green!50]{#1}}
\newcommand{\cetodo}[1]{\todo[inline, color=orange!50]{#1}}

\begin{document}
\title{Running experiments with confidence and sanity}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{
  Martin Aumüller\inst{1}\orcidID{0000-0002-7212-6476} \and
  Matteo Ceccarello\inst{2}\orcidID{0000-0003-2783-0218}}
%
\authorrunning{M. Aumüller and M. Ceccarello}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{
  IT University of Copenhagen,
  Denmark\\
  \email{maau@itu.dk}
  \and
  Free University of Bozen, Italy\\
  \email{mceccarello@unibz.it}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Analyzing data from large experimental suites is a daily task for
anyone doing experimental algorithmics.
In this paper we report on several approaches we tried for this 
seemingly mundane task, reflecting on the many errors and consequent
mishaps.

We conclude by proposing a workflow, which can be implemented using several
tools, that allows to analyze experimental data with confidence.

\keywords{Experimental algorithmics; Experimental analysis; data analysis}
\end{abstract}

\section{Introduction}

As we shall see, the literature is mainly concerned with the design 
and analysis of
experiments, with the communication of the results,
and with reproducibility.
In this paper, instead, we report on our experience with the day to day
tasks that have to be carried out in between those three tasks, and
the approaches we developed to tackle the perils and frustrations of this 
often menial work.

\section{Related work}

\subsection{Experimental algorithmics}

McGeoch and Moret~\cite{DBLP:journals/sigact/McGeochM99} give advice on how
to present experimental result, but don't describe how to run the experiments
themselves.
Sanders~\cite{DBLP:conf/dagstuhl/Sanders00} describes how to report results
in papers.

The book by McGeoch~\cite{DBLP:reference/algo/McGeoch08} describes a methodology
for running experiments.

Have a look at~\cite{DBLP:journals/jucs/MoretS01} and~\cite{DBLP:conf/dimacs/Moret99}.

There is a 2010 book~\cite{DBLP:books/sp/2010BCPP}.

Also read~\cite{DBLP:series/ncs/Bartz-BeielsteinP14}.

From McGeoch~\cite{DBLP:journals/cacm/McGeoch07} we have a motivation for complex
experimental analyses:
\begin{center}
\begin{minipage}{0.8\textwidth}
  In addition, in the past few decades, experimental methodology in this
  context has evolved into some-thing resembling a classic “laboratory
  experiment” for analyzing algorithms. This approach emphasizes highly
  controlled parameters, carefully placed data-collection probes, and
  sophisticated data analysis.Rather than simply measure performance, the
  goal of the experiment is to generalize away from context-specific
  measurements and build insight into fundamental structures and properties.

  Indeed, since experiments can be used to measure any aspect of
  algorithm/program performance, not just CPU times, this approach can
  stimulate new forms of interplay between theory and practice. The term
  “experimental algorithmics” describes this new approach to algorithm design
  and analysis, combining theoretical questions and motivations with
  empirical research methods. The related term “algorithm engineering” is
  almost synonymous but emphasizes design over analysis.
\end{minipage}
\end{center}

There has been a Dagstuhl seminar on the topic of 
experimental evaluation~\cite{DBLP:conf/dagstuhl/2000ea}.

Demetrescu et al.~\cite{DBLP:conf/dagstuhl/DemetrescuFIN00} worked on
the topic of visualizing experimental results in experimental algorithmics.

\subsection{Reproducibility}

All the works like sumatra, ReproZip, docker, that capture the environment for
replicability.

Then there are the tools for literate programming.

\section{Challenges in running large scale experimental evaluation}

\section{Case study}

\cetodo{Here we describe our toy project. I think it's better to describe
it here in order to use it as a running example in the next subsections.}

\section{Approaches to experimental evaluation}

\subsection{Manage the datasets efficiently}
\subsection{Manage the experimental configurations clearly}
\subsection{Version everything}
\subsection{Manage the experimental results thoughtfully}
\subsection{Don't repeat yourself}

\section{Conclusions}

\bibliographystyle{alpha}
\bibliography{references}

\appendix

\section{Notes}

There are several challenges when running large experimental suites
for the evaluation of algorithms.

\begin{itemize}
\item Oftentimes the algorithm is modified in response to the results
  of the experimental study. It is important not to mix results from
  different algorithms, and from different code implementations of the
  same algorithm.
\item Usually running the experiments takes a long time, and we don't
  want to re-run all the experiments every time. However, when
  re-using results from old experiments, we need to be sure that we
  are not including undesired and obsolete results.
\item There are two types of experiments: the ones you do during the
  development of the code, which are useful to find out the most
  appropriate parameter ranges, shake out bugs, check assumptions,
  etc; The second kind of experiments is instead devoted to the
  collection of the results of the study. This means that experiments
  are run over a long span of time. It is very important that results
  obtained with different code versions are not mixed.
\item An experiment is run multiple times with the same parameter
  configuration (including the random seed), but only the most recent
  one is interesting.
\item Experiments are run on a machine (or cluster of machines) and then
  analyzed in another.
\item There are \emph{many} experiemnts.
\item The analysis is carried out with different tools (python, R) and
  should run reasonably efficiently
\end{itemize}

Here follows the \emph{keys for a successful experimental evaluation}

\paragraph{Manage the experimental results thoughtfully}

\begin{itemize}
\item Collect everything you need, but nothing more: it's expensive to process too
  much data you don't need
\item Implement a mechanism to keep track of only the latest version of an
  experiment. This is crucial to be able to re-run parameter configurations
  without being worried that some old results are sneaking in. You may also
  need some mechanism to clean up old configurations (and then also some
  mechanism to back up)
\item Keep the analysis efficient (use tools like make)
\item Using a database helps managing changes with database migrations.
\end{itemize}

\paragraph{Manage the experimental configurations sanely}

\begin{itemize}
\item Never run experiments from the command line, keep that for testing.
\item Have a file describing all the experiments that will go into the paper. This
  can be:
  \begin{itemize}
  \item A file in a declarative language such as YAML. The pros are that it is
    declarative, the cons are that you have to implement the logic to use it
    anyway.
  \item A simple bash script might be a better option: loops are used to test
      different combinations of parameters, and switch statements are used to
      test experiment groups.
  \item A third alternative is to make the code accept configuration file with all
      the parameters to test. The problem is then that we need to implement the
      logic to handle all these runs, which can be very cumbersome an is
      ultimatly wasted work.
  \end{itemize}
\item Allow to skip rerunning parameter configurations, so to avoid continuous
  editing of the experimental files. Ideally, you should have a way for your
  code to telle whether an experiment has already been run, giving the option
  to skip runinng it or to "override" the result, actually just appending the
  new result.
\end{itemize}

\paragraph{Manage the datasets efficiently}

\begin{itemize}
\item \emph{No manual downloading and preprocessing}. At the very minimum, you want a
  script to download and preprocess your data for you
\item In my experience, `bash` quickly becomes very tangled, and `python` is a mess
  to set up if you have to move on a new machine. Embedding the preprocessing
  (and downloading) code in the experiment's code is the best way to go. You
  get a single development environment to set up, a single entry point for the
  experiments (no need to run a script to preprocess and distribute data as a
  separate step).
\item Maybe the datasets should be versioned?
\item There should be a command to list all available datasets, and remove the ones
  that are no longer needed.
\end{itemize}

\paragraph{The code}

Not just from the point of view of implementation, but maybe even more
prominently from the point of view of usage. That is to say:

\begin{itemize}
\item Keep parameters at the bare minimum. If a parameter never changes across
  experiments, then remove it from the tunable options. That is: keep the
  interface minimal
\item Don't use environment variables. They seem neat and convenient, but they soon
  go out of eyesight and you may forget that some are set to some value. It's
  way better to have all the parameters specified as command line arguments:
  having to repeat them every time is a good reminder to keep the interface
  minimal.
\item Use assertions everywhere, possibly active just during debug. This means that
  you should have a test dataset that can run in a reasonable time in debug
  mode. The problem is that often the problems arise when using large datasets
  that can be handled just in release mode.
\end{itemize}

\paragraph{Version everything}

Keeping the source code under version control might not be sufficient, since
source code revisions (in isolation) lack semantic meaning: you cannot tell
if a version is more recent than another without looking at the VCS DAG.
Furthermore, there are several components that may need versioning in
a project:
\begin{itemize}
  \item Algorithms implementation
  \item The schemas of the database for reporting results (or the fields of 
    the CSV files, if we don't use a database)
  \item Datasets, for which the preprocessing might evolve in time in 
    response to fixing bugs
\end{itemize}
All these components can evolve independently from one another, even within each category
(we can update just one of the many algorithms and datasets under consideration).
These changes should be recorded, but we don't want to re-run all the experiments just because
one dataset was updated. Rather, we would like to re-run only those experiments involving it.
\cetodo{This is rather similar to \texttt{make}, so we keep re-inventing the wheel, 
or the necessities are always the same.}
For this purpose, using the git version of the code is not well suited to this, as it is
shared between all the components.
\end{document}

